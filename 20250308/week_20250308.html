<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI周报(0308-0314)</title>
    <!-- 引入Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background-color: #f8f9fa;
            color: #333;
            line-height: 1.8;
        }
        h1, h2, h3 {
            color: #2c3e50;
            margin-top: 1.5rem;
            margin-bottom: 1rem;
        }
        h1 {
            font-size: 2rem;
            font-weight: bold;
        }
        h2 {
            font-size: 2rem;
            font-weight: 600;
        }
        h3 {
            font-size: 1.5rem;
            font-weight: 500;
        }
        .quote {
            font-style: italic;
            color: #555;
            margin: 2rem 0;
            padding: 1rem;
            border-left: 4px solid #007BFF;
            background-color: #f1f1f1;
        }
        .section {
            background-color: #fff;
            border-radius: 8px;
            padding: 1.5rem;
            margin-bottom: 2rem;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }
        .section ul {
            padding-left: 1.5rem;
        }
        .section ul li {
            margin-bottom: 0.5rem;
        }
        .footer {
            margin-top: 3rem;
            text-align: center;
            color: #777;
            padding: 1rem;
            background-color: #f8f9fa;
            border-top: 1px solid #ddd;
        }
        a {
            color: #007BFF;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        .img-fluid {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 1rem 0;
        }
        .responsive-img {
            max-width: 90%;
            height: auto;
        }
    </style>
</head>
<body>
    <div class="container my-5">
        <h1 class="text-center mb-4">AI周报(0308-0314)</h1>

        <div class="section">
            <h3>1、AI应用(toC)——腾讯元宝和腾讯文档正式打通，提升协作办公体验</h3>
            <p>3月13日，腾讯宣布其智能助手腾讯元宝与腾讯文档实现深度打通。用户能通过元宝一键上传包括表格、文档、PPT、PDF、思维导图在内的多种腾讯文档格式。元宝无需额外转换即可直接理解和处理这些文档。同时，元宝生成的回答也支持一键导出为腾讯文档，方便用户进行后续编辑、二次创作以及分享。</p>
            <p>该功能的实现，极大地简化了办公流程。例如，当用户需要处理多种复杂文档时，可直接上传至元宝进行整理、分析和优化，然后一键导出为结构清晰、规范的腾讯文档，以便用于会议演示或团队协作。用户还可以将腾讯文档、微信文件以及本地文件一起上传解析，彻底告别在多个窗口之间来回切换的烦恼。</p>
            <a href="https://mp.weixin.qq.com/s/ery7YyWyk5IIou6sJppm5Q" target="_blank">腾讯元宝X腾讯文档：通了</a><br>
            
<!--             <div class="center">
                <img src="./京东健康.jpg" alt="示例图片" class="responsive-img">
            </div> -->
        </div>

        <div class="section">
            <h3>2、AI应用(toC)——阿里推出AI全能助手，超级应用入口整合深度对话思考能力，满足多元需求</h3>
            <p>3月13日阿里宣布，基于阿里通义领先的推理及多模态大模型，将旗下智能搜索产品夸克升级为新的旗舰AI应用，使用界面是一个形式极简、用于输入指令的<strong>“AI超级框”</strong>。比起目前市场上的主流的对话机器人形式更进一步，夸克将AI对话、深度思考、深度搜索、深度研究、深度执行整合到一个搜索对话框内，一站式满足用户工作、学习、生活各项需求。</p>
            <p>新夸克号称是用户的“AI全能助手”，只需在“AI超级框”输入指令，夸克智能中枢系统将自动识别用户意图并进行深度思考、规划，完成包括AI搜索、写作、文生图，以及制作PPT、学术研究、AI搜题、AI健康问答、旅行计划等任务。</p>
            <p>实际上，夸克在23年已自研并推出大模型，但由于同为阿里体系下的通义千问系列珠玉在前，而使得市场上存在疑问之声。而本次夸克升级明确了是基于阿里通义系列的模型。同时，阿里巴巴官方宣布未来通义系列模型的最新成果都将第一时间接入夸克，持续更新和优化其AI服务功能。通过内部整合优势资源，夸克作为一个<strong>超过2亿用户</strong>的应用超级入口接入阿里通义系列最先进的模型，有助于提升用户的AI使用体验，并在未来可能成为类似字节豆包那样的代表性C端产品。</p>
            <a href="https://mp.weixin.qq.com/s/YH_vd6DaA4nEc_iypiPs0A" target="_blank">你好 新夸克</a><br>
            <a href="https://mp.weixin.qq.com/s/dgWoJpK3wajKz8HULc3n5g" target="_blank">阿里巴巴正式发布AI旗舰应用——新夸克</a><br>
        </div>

        <div class="section">
            <h3>3、AI应用(toB/C)——OpenAI推出ResponsesAPI,统一工具接口简化智能体编排</h3>
            <p>一般情况下开发一款智能体或多功能的应用时，需要从不同来源拼凑各种底层API，涉及到出入参的调整和API结果解析方式不同，从而需要针对各个API定制化开发代码，<strong>任务越复杂，定制化程度越可能出错</strong>。而通过统一的API接口，开发者可以更轻松地集成多种工具和服务，<strong>减少了集成多个API的复杂性</strong>。ResponsesAPI⁠就是这样一个统一接口，只需调用一次，就将多个Chat Completions API(即调用大模型对话的API)与Assistants API工具相结合来构建智能体。</p>
            <p>同时ResponsesAPI已经内置了三个工具：</p>
            <ul>
                <li> <strong>网络搜索</strong>: 从网络快速获取最新公开信息资源形成答案，同时带有清晰且相关的引文信息；</li>
                <li> <strong>文件搜索</strong>: 从多种类型的文档中检索相关信息，支持元数据过滤和自定义的相关性排序，可用于智能客服、法律顾问等场景；</li>
                <li> <strong>计算机使用</strong>: 可捕获模型生成的鼠标和键盘操作，将其转换为可执行命令，可以用来验证页面或者数据输入等比较固定流程的任务。</li>
            </ul>
            <p>近期Anthropic推出的MCP也同样是为了解决复杂智能体集成大量API比较繁琐的问题，不同的是，MCP本身是一项协议和通用标准，<strong>类似于App Store的第三方市场</strong>，需要开发社区公认和协作；而ResponsesAPI则类似于和厂商生态绑定的一站式服务，是<strong>OpenAI技术生态下的专有解决方案</strong>。这两者的目标趋同，但在智能体时代也可能存在竞争关系。</p>
            <a href="https://mp.weixin.qq.com/s/GY-zpjbQlA0FdNLAykoEvA" target="_blank">OpenAI突然发布智能体API！支持网络和文件搜索以及computer use</a><br>
        </div>
        
        <div class="section">
            <h3>4、AI模型(底座大模型)——字节豆包文生图技术公开，扎实技术底蕴提升用户体验</h3>
            <p>3月12日豆包大模型团队正式发布文生图技术报告，首次公开Seedream2.0图像生成模型技术细节，覆盖数据构建、预训练框架、 后训练RLHF全流程。Seedream2.0于24年12月在豆包APP和即梦上线，服务上亿C端用户。相比与Ideogram、Midjourney等主流模型，Seedream2.0更好解决了<strong>文本生成和渲染能力欠佳</strong>、<strong>对中国文化理解不足等诸多实际问题</strong>，美感、指令遵循等能力有整体提升。</p>
            <p>其中非常创新的一点的是通过特定目标的RLHF对齐技术(即强化学习RL)来增强模型的整体性能，其中包括三个让专项能力突破的奖励模型：<strong>图像文本对齐RM</strong>、<strong>美学RM</strong>、<strong>文本渲染RM</strong>，其中文本渲染RM在检测到“文本生成”类标签时，模型将强化字符细节优化能力，以提升汉字生成准确率。</p>
            <p>豆包大模型团队提出，“伴随 2025 年强化学习浪潮兴起，团队将持续探索基于强化学习的优化机制，包括如何更好地设计奖励模型及数据构建方案。”Seedream2.0的奖励模型如此设计也是为其他工作在全场景RL和多目标RL提供了非常成功的参照。</p>
            <a href="https://mp.weixin.qq.com/s/3E4s2c7TcWQ_g_6DdJPJkQ" target="_blank">豆包文生图技术报告发布！数据处理、预训练、RLHF全流程公开</a><br>
            <a href="https://team.doubao.com/zh/tech/seedream" target="_blank">技术展示</a><br>
        </div>

        <div class="footer">
            <p>AI周报(0308-0314) - 结束</p>
            <a href="../index.html">返回首页</a>
        </div>
    </div>

    <!-- 引入Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>
